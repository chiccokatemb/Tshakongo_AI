
# Tshakongo_AI â€” Option E (Complet)

Assistant vocal **100% local** pour Raspberry Pi :
- ğŸ™ï¸ Wake-word **tshakongo**, STT (Vosk), TTS (Piper)
- ğŸ§  Neurone local (llama.cpp / Ollama) + cache sÃ©mantique
- ğŸ’¬ Dialogue â†’ actions (MQTT, robot, fichiers, scripts, nav)
- ğŸ§­ Navigation autonome (LiDAR LD06) simple Ã©vitement
- ğŸ›¡ï¸ Vision sÃ©curitÃ© (armes/chiens) + snapshots + MQTT alert
- ğŸ“œ Scripts & Apps (Flask/FastAPI/CLI) + multi-lang (Docker)
- â±ï¸ Scheduler (APScheduler)
- ğŸ” Mise Ã  jour GitHub

## DÃ©marrage
```bash
cd ~/Tshakongo_AI
bash scripts/manage.sh setup
bash scripts/manage.sh run
# UI: http://<IP_DU_PI>:8080/
```

## Wake-word & Voix
Dans `app/config.yaml` :
```yaml
speech:
  wakeword: "tshakongo"
  wakeword_sensitivity: 0.70
  stt_model_url: "https://alphacephei.com/vosk/models/vosk-model-small-fr-0.22.zip"
  stt_model_dir: "models/vosk-fr"
  tts_voice: "fr"
```
Installe:
```bash
pip install openwakeword onnxruntime numpy sounddevice
sudo apt install piper
```

## MQTT aliases (exemples)
```yaml
mqtt:
  aliases:
    lumiere_salon: "homeassistant/light/salon/set"
    door_main: "homeassistant/lock/porte/set"
    scene_night: "homeassistant/scene/night/set"
    scene_party: "homeassistant/scene/party/set"
```

## Vision sÃ©curitÃ© â€” modÃ¨les ONNX
Place des modÃ¨les YOLO export ONNX (armes/chiens) et configure si besoin dans le code (`VisionSecure`).  
Captures: `/home/pi/tshakongo/alerts/` â€” MQTT: `tshakongo/alerts`.

## Nav autonome
- `/api/nav/autonomous/start|stop` â€” simple Ã©vitement (min distance)

## Code multi-langages
`POST /api/code/run` avec `{language, code, args, network}`

## Mise Ã  jour
Bouton UI **Mise Ã  jour** ou `bash scripts/manage.sh update`
